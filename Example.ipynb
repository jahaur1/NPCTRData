{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e39ab2-9341-4ca1-b180-cedfd76cead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from cross_Transformer_nys import Trans_C as Trans_C_nys\n",
    "from cross_Transformer import Trans_C\n",
    "from loss.dilate_loss import DilateLoss\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "# Set random seed\n",
    "\n",
    "def seed_torch(seed=2025):\n",
    "    random.seed(seed)   # Python的随机性\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)    # 设置Python哈希种子，为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)   # numpy的随机性\n",
    "    torch.manual_seed(seed)   # torch的CPU随机性，为CPU设置随机种子\n",
    "    torch.cuda.manual_seed(seed)   # torch的GPU随机性，为当前GPU设置随机种子\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子\n",
    "    torch.backends.cudnn.benchmark = False   # if benchmark=True, deterministic will be False\n",
    "    torch.backends.cudnn.deterministic = True   # 选择确定性算法\n",
    "\n",
    "seed_torch()\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}.\")\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from cycler import cycler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from RevIN import RevIN\n",
    "rcParams['figure.figsize'] = 13, 4\n",
    "\n",
    "# Box\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.left'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['axes.prop_cycle'] = cycler(color=['navy','goldenrod'])\n",
    "\n",
    "# Grid and axis thickness, color\n",
    "rcParams['axes.linewidth'] = 1\n",
    "rcParams['axes.edgecolor'] = '#5B5859'\n",
    "rcParams['axes.ymargin'] = 0\n",
    "rcParams['axes.grid'] = True\n",
    "rcParams['axes.grid.axis'] = 'y'\n",
    "rcParams['axes.axisbelow'] = True\n",
    "rcParams['grid.color'] = 'grey'\n",
    "rcParams['grid.linewidth'] = 0.5\n",
    "\n",
    "# xticks, yticks\n",
    "rcParams['ytick.major.width'] = 0\n",
    "rcParams['ytick.major.size'] = 0\n",
    "rcParams['ytick.color'] = '#393433'\n",
    "rcParams['xtick.major.width'] = 1\n",
    "rcParams['xtick.major.size'] = 3\n",
    "rcParams['xtick.color'] = '#393433'\n",
    "\n",
    "# Line thickness\n",
    "rcParams['lines.linewidth'] = 1.5\n",
    "\n",
    "# Saving quality\n",
    "rcParams['savefig.bbox'] = 'tight'\n",
    "rcParams['savefig.dpi'] = 500\n",
    "rcParams['savefig.transparent'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecca9611-4ede-498f-a017-73b36311f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "site=708\n",
    "csv_path = Path(f\"E:\\Project2024\\canada\\DATA1\\WTS{site}_1.csv\")\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54cc708-285c-4a46-8adf-2e9389ce846e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Qrate</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Day_of_month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Week</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-09-09 12:00:00</th>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-09 13:00:00</th>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-09 14:00:00</th>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-09 15:00:00</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-09 16:00:00</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Qrate  Rain  Day_of_month  Day_of_week  Hour  Week\n",
       "Date                                                                    \n",
       "2014-09-09 12:00:00  0.0186   0.0             9            1    12    37\n",
       "2014-09-09 13:00:00  0.0188   0.0             9            1    13    37\n",
       "2014-09-09 14:00:00  0.0188   0.0             9            1    14    37\n",
       "2014-09-09 15:00:00  0.0185   0.0             9            1    15    37\n",
       "2014-09-09 16:00:00  0.0185   0.0             9            1    16    37"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "# Day in a month\n",
    "df[\"Day_of_month\"] = df.Date.apply(lambda x: x.day)\n",
    "# Day in a week\n",
    "df[\"Day_of_week\"] = df.Date.apply(lambda x: x.dayofweek)\n",
    "# 24-hour based\n",
    "df[\"Hour\"] = df.Date.apply(lambda x: x.hour)\n",
    "# Week in a year\n",
    "df[\"Week\"] = df.Date.apply(lambda x: x.week)\n",
    "# Set \"DateTime\" column as row index\n",
    "df = df.set_index(\"Date\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830c4ffb-da22-4291-8871-e71c60b6c963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (26627, 6)\n",
      "Validation Shape: (8750, 6)\n",
      "Testing Shape: (8750, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "in_seq_len=past_len=24# How far to look back\n",
    "out_seq_len=pred_len =24# How far to look forward\n",
    "forecast_length = out_seq_len# Hours ahead to predict\n",
    "enc_in_size = df.shape[1] # Number of input features + target feature\n",
    "dec_in_size = 6 # Number of known future features + target feature\n",
    "output_size = 1 # Number of target features\n",
    "hidden_size = 128 # Dimensions in hidden layers\n",
    "num_layers = 2# Number of hidden layers\n",
    "patch_len=6\n",
    "num_epochs = 300\n",
    "learning_rate = 1e-3\n",
    "es_patience = 20\n",
    "lr_patience = 10\n",
    "model_save_path = \"checkpoint_seq2seq.pth\"\n",
    "df['Qrate'] = np.log1p(df['Qrate'])\n",
    "df['Rain'] = np.log1p(df['Rain'])\n",
    "# 保存原始均值和标准差\n",
    "scaler_qrate = StandardScaler()\n",
    "scaler_rain = StandardScaler()\n",
    "df['Qrate'] = scaler_qrate.fit_transform(df[['Qrate']])\n",
    "df['Rain'] = scaler_rain.fit_transform(df[['Rain']])\n",
    "\n",
    "# Move target to the last column\n",
    "target_feature = \"Qrate\"\n",
    "df.insert(len(df.columns)-1, target_feature, df.pop(target_feature))\n",
    "data = df.values\n",
    "\n",
    "testNum = validationNum = 8750\n",
    "total_rows = len(df)\n",
    "assert total_rows == testNum * 2 + (train_df_rows := total_rows - 2*testNum), \"数据行数不支持这样的划分\"\n",
    "data_train = df[:train_df_rows].copy()\n",
    "data_val = df[train_df_rows:train_df_rows+validationNum].copy()\n",
    "data_test = df[train_df_rows+validationNum:train_df_rows+validationNum+testNum].copy()\n",
    "print(\"Training Shape:\", data_train.shape)\n",
    "print(\"Validation Shape:\", data_val.shape)\n",
    "print(\"Testing Shape:\", data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7139680-d9ea-4d64-894b-25b033f33352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([32, 24, 6])\n",
      "Target shape: torch.Size([32, 24])\n"
     ]
    }
   ],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, past_len=past_len, pred_len=pred_len):\n",
    "        # Create data sequences\n",
    "        data_len = data.shape[0]\n",
    "        # 27725\n",
    "        X, Y = list(), list()\n",
    "\n",
    "        for i in range(data_len):\n",
    "            input_end = i + past_len\n",
    "\n",
    "            output_end = input_end + pred_len\n",
    "\n",
    "            # check if we are beyond the dataset\n",
    "            if output_end > data_len:\n",
    "                break\n",
    "            else:\n",
    "                X.append(data[i:input_end])\n",
    "                Y.append(data.iloc[input_end:output_end, -1].values)\n",
    "\n",
    "        # Shape (samples, seq_len, features)\n",
    "        self.X = np.array(X)\n",
    "        # Shape (samples, seq_len, ) : univariate\n",
    "        self.Y = np.array(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the size of the dataset\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return one sample from the dataset\n",
    "        features = self.X[idx]\n",
    "        target = self.Y[idx]\n",
    "        return features, target\n",
    "train_dataset = TimeSeriesDataset(data_train)\n",
    "val_dataset = TimeSeriesDataset(data_val)\n",
    "test_dataset = TimeSeriesDataset(data_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "X, y = next(iter(train_loader))\n",
    "\n",
    "print(\"Features shape:\", X.size())\n",
    "print(\"Target shape:\", y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa9bd9b-1617-4242-9513-009e7650a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Filter(nn.Module):\n",
    "    def __init__(self, channel=1, kernel_size=12):\n",
    "        super(Filter, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv1d(channel, channel, kernel_size=kernel_size, stride=1,\n",
    "                              padding=int(kernel_size // 2), padding_mode='replicate', bias=True, groups=channel)\n",
    "        self.conv.weight = nn.Parameter(\n",
    "            (1 / kernel_size) * torch.ones([channel, 1, kernel_size]))\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.conv(inp.transpose(1, 2)).transpose(1, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Fredformer_backbone(nn.Module):\n",
    "    def __init__(self, context_window: int, target_window: int, ablation: int, mlp_drop: float, use_nys: int, output: int, mlp_hidden: int, cf_dim: int,\n",
    "                 cf_depth: int, cf_heads: int, cf_mlp: int, cf_head_dim: int, cf_drop: float, c_in: int,\n",
    "                 patch_len: int, stride: int, d_model: int,\n",
    "                 head_dropout=0, padding_patch=None, individual=False, revin=False, affine=True, subtract_last=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "        self.use_nys = use_nys\n",
    "        self.ablation = ablation\n",
    "        self.beta=0.2\n",
    "        # RevIn\n",
    "\n",
    "        self.output = output\n",
    "        # Patching\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.padding_patch = padding_patch\n",
    "        self.targetwindow = target_window\n",
    "        self.horizon = self.targetwindow\n",
    "        patch_num = int((context_window - patch_len) / stride + 1)\n",
    "        self.norm = nn.LayerNorm(patch_len)\n",
    "        self.Filter = Filter(c_in, kernel_size=25)\n",
    "        # print(\"depth=\",cf_depth)\n",
    "        # Backbone\n",
    "        self.re_attn = True\n",
    "        if self.use_nys == 0:\n",
    "            self.fre_transformer = Trans_C(dim=cf_dim, depth=cf_depth, heads=cf_heads, mlp_dim=cf_mlp,\n",
    "                                           dim_head=cf_head_dim, dropout=cf_drop, patch_dim=patch_len * 2,\n",
    "                                           horizon=self.horizon * 2, d_model=d_model * 2)\n",
    "        else:\n",
    "            self.fre_transformer = Trans_C_nys(dim=cf_dim, depth=cf_depth, heads=cf_heads, mlp_dim=cf_mlp,\n",
    "                                               dim_head=cf_head_dim, dropout=cf_drop, patch_dim=patch_len * 2,\n",
    "                                               horizon=self.horizon * 2, d_model=d_model * 2)\n",
    "\n",
    "        # Head\n",
    "        self.head_nf_f = d_model * 2 * patch_num  # self.horizon * patch_num#patch_len * patch_num#1408\n",
    "        self.n_vars = c_in\n",
    "        self.individual = individual\n",
    "        self.head_f1 = Flatten_Head(self.individual, self.n_vars, self.head_nf_f, target_window,\n",
    "                                    head_dropout=head_dropout)\n",
    "        self.head_f2 = Flatten_Head(self.individual, self.n_vars, self.head_nf_f, target_window,\n",
    "                                    head_dropout=head_dropout)\n",
    "\n",
    "        self.ircom = nn.Linear(self.targetwindow * 2, self.targetwindow)\n",
    "        self.rfftlayer = nn.Linear(self.targetwindow * 2 - 2, self.targetwindow)\n",
    "        self.final = nn.Linear(self.targetwindow * 2, self.targetwindow)\n",
    "        self.inn = INN_all(num_layers=1)\n",
    "        self.Linear1=nn.Linear(c_in, d_model*2)\n",
    "        self.adaptive= nn.Parameter(torch.ones(2))\n",
    "\n",
    "\n",
    "        # break up R&I:\n",
    "        self.get_r = nn.Linear(d_model * 2, d_model * 2)\n",
    "        self.get_i = nn.Linear(d_model * 2, d_model * 2)\n",
    "        self.output1 = nn.Linear(target_window, target_window)\n",
    "\n",
    "        # ablation\n",
    "        self.input = nn.Linear(c_in, patch_len * 2)\n",
    "        self.outpt = nn.Linear(d_model * 2, c_in)\n",
    "        self.abfinal = nn.Linear(patch_len * patch_num, target_window)\n",
    "        self.fc=nn.Linear(6,1)\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.CIAA=CIAA(d_model, n_heads=8, use_cls=True)\n",
    "\n",
    "    def fftpatch(self,tensor, patch_len, stride):\n",
    "\n",
    "        fft_result = torch.fft.fft(tensor)\n",
    "\n",
    "        real_part = fft_result.real\n",
    "        imag_part = fft_result.imag\n",
    "\n",
    "        real_unfolded = real_part.unfold(dimension=-1, size=patch_len,\n",
    "                                         step=stride)  # [bs x nvars x patch_num x patch_len]\n",
    "        imag_unfolded = imag_part.unfold(dimension=-1, size=patch_len,\n",
    "                                         step=stride)  # [bs x nvars x patch_num x patch_len]\n",
    "\n",
    "        return real_unfolded, imag_unfolded\n",
    "    def forward(self,z):\n",
    "        x = z - self.beta * self.Filter(z)\n",
    "        x1, x2 = self.fftpatch(x.permute(0, 2, 1), self.patch_len, self.stride)#32,6,4,6\n",
    "\n",
    "\n",
    "        z = z.permute(0, 2, 1)  # [bs x seq_len x nvars] -> [bs x nvars x seq_len]\n",
    "        z1, z2 = self.fftpatch(z, self.patch_len, self.stride)\n",
    "        #z1 = z1.unfold(dimension=-1, size=self.patch_len, step=self.stride)  # z1: [bs x nvars x patch_num x patch_len]\n",
    "        #z2 = z2.unfold(dimension=-1, size=self.patch_len,step=self.stride)  # z2: [bs x nvars x patch_num x patch_len]32,6,4,6\n",
    "\n",
    "        # for channel-wise_1\n",
    "        z1 = z1.permute(0, 2, 1, 3)#32,4,6,6\n",
    "        z2 = z2.permute(0, 2, 1, 3)\n",
    "\n",
    "        # model shape\n",
    "        batch_size = z1.shape[0]\n",
    "        patch_num = z1.shape[1]\n",
    "        c_in = z1.shape[2]\n",
    "        patch_len = z1.shape[3]\n",
    "\n",
    "        x=torch.cat((x1, x2), 1)\n",
    "        x=self.inn(x)\n",
    "        x=self.Linear1(x)\n",
    "\n",
    "        # proposed\n",
    "        z1 = torch.reshape(z1, (batch_size * patch_num, c_in, z1.shape[-1]))  # z: [bs * patch_num,nvars, patch_len]\n",
    "        z2 = torch.reshape(z2, (batch_size * patch_num, c_in, z2.shape[-1]))  # z: [bs * patch_num,nvars, patch_len]128,6,6\n",
    "\n",
    "        z = self.fre_transformer(torch.cat((z1, z2), -1))#128,6,128\n",
    "\n",
    "        z = torch.reshape(z, (batch_size, patch_num, c_in, z.shape[-1]))\n",
    "        z = z.permute(0, 2, 1, 3)#32,4,6,128->32,6,4,128\n",
    "        z = x * self.adaptive[0] + z * self.adaptive[1]#32,6,4,128\n",
    "        runoff = z[:, 1:, :,:]\n",
    "        x=self.CIAA(runoff,x)\n",
    "        x= torch.fft.ifft(x)\n",
    "        xr = x.real\n",
    "        xi = x.imag\n",
    "        x = self.ircom(torch.cat((xr, xi), -1))\n",
    "\n",
    "\n",
    "        '''\n",
    "        z1 = self.get_r(z)\n",
    "        z2 = self.get_i(z)\n",
    "        \n",
    "        z1 = self.get_r(z)\n",
    "        z2 = self.get_i(z)\n",
    "\n",
    "        z1 = torch.reshape(z1, (batch_size, patch_num, c_in, z1.shape[-1]))\n",
    "        z2 = torch.reshape(z2, (batch_size, patch_num, c_in, z2.shape[-1]))\n",
    "\n",
    "        z1 = z1.permute(0, 2, 1, 3)  # z1: [bs, nvars， patch_num, horizon]32,4,6,128->32,6,4,128\n",
    "        z2 = z2.permute(0, 2, 1, 3)\n",
    "        \n",
    "\n",
    "\n",
    "        #z1 = torch.flatten(z1, start_dim=2)\n",
    "        z1 = self.head_f1(z1)  # z: [bs x nvars x target_window]\n",
    "        z2 = self.head_f2(z2)  # z: [bs x nvars x target_window]\n",
    "\n",
    "        z = torch.fft.ifft(torch.complex(z1, z2))\n",
    "        zr = z.real\n",
    "        zi = z.imag\n",
    "        z = self.ircom(torch.cat((zr, zi), -1))\n",
    "        \n",
    "        z = z.permute(0, 2, 1)\n",
    "        z=self.fc(z)\n",
    "        '''\n",
    "        return x.unsqueeze(1)\n",
    "class INN(nn.Module):\n",
    "    def __init__(self, input, output, ratio):\n",
    "        super(INN, self).__init__()\n",
    "        hidden_dim = int(input * ratio)\n",
    "        self.bottleneckBlock = nn.Sequential(\n",
    "            nn.Conv2d(input, hidden_dim, 1, bias=False),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, output, 1, bias=False),\n",
    "            nn.BatchNorm2d(output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bottleneckBlock(x)\n",
    "        if x.shape[1] == out.shape[1]:\n",
    "            out = out + x\n",
    "        return out\n",
    "\n",
    "class Feature(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Feature, self).__init__()\n",
    "        self.phi = INN(input=32, output=32, ratio=2)  # Adjusted parameter names\n",
    "        self.seta = INN(input=32, output=32, ratio=2)\n",
    "\n",
    "    def forward(self, f1, f2):\n",
    "        f2 = f2 + self.phi(f1)\n",
    "        f1 = f1 + self.seta(f2)\n",
    "        return f1, f2\n",
    "\n",
    "class INN_all(nn.Module):\n",
    "    def __init__(self, num_layers=None):\n",
    "        super(INN_all, self).__init__()\n",
    "        INN_layers = [Feature() for _ in range(num_layers)]\n",
    "        self.net = nn.Sequential(*INN_layers)\n",
    "        self.shffle = nn.Conv2d(12, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fusion = nn.Conv2d(64, 12, kernel_size=3, stride=1, padding=1)\n",
    "        self.Merge = nn.Conv2d(12, 6, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def separate(self, x):\n",
    "        f1, f2 = x[:, :x.shape[1] // 2], x[:, x.shape[1] // 2:x.shape[1]]\n",
    "        return f1, f2\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1, f2 = self.separate(self.shffle(x))\n",
    "        for layer in self.net:\n",
    "            f1, f2 = layer(f1, f2)\n",
    "        f_out = self.fusion(torch.cat((f1, f2), dim=1))#32,12,4,6\n",
    "        f_out = self.Merge(f_out)#32,6,4,6\n",
    "        return f_out\n",
    "\n",
    "class Flatten_Head(nn.Module):\n",
    "    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.individual = individual\n",
    "        self.n_vars = n_vars\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear1 = nn.Linear(nf, nf)\n",
    "        self.linear2 = nn.Linear(nf, nf)\n",
    "        self.linear3 = nn.Linear(nf, nf)\n",
    "        self.linear4 = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n",
    "\n",
    "        x = self.flatten(x)#32,6,512\n",
    "        x = F.relu(self.linear1(x)) + x\n",
    "        x = F.relu(self.linear2(x)) + x\n",
    "        #x = F.relu(self.linear3(x)) + x\n",
    "        x = self.linear4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CIAA(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, use_cls=True):\n",
    "        \"\"\"\n",
    "        Applies multi-head attention between runoff and exogenous variables.\n",
    "\n",
    "        :param d_model: Feature dimension (e.g., 128)\n",
    "        :param n_heads: Number of attention heads\n",
    "        :param use_cls: Whether to use [CLS] token for output aggregation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.use_cls = use_cls\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(512, d_model)\n",
    "        self.k_proj = nn.Linear(512, d_model)\n",
    "        self.v_proj = nn.Linear(512, d_model)\n",
    "\n",
    "        # Final output projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Dropout and normalization\n",
    "        self.attn_dropout = nn.Dropout(0.1)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Optional [CLS] token\n",
    "        if use_cls:\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, 1, 1, 512))\n",
    "            self.linear=nn.Linear(d_model,24)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        \"\"\"\n",
    "        :param x: Runoff input tensor, shape: [B, nvars_x, H, d_model]\n",
    "        :param z: Exogenous input tensor, shape: [B, nvars_z, H, d_model]\n",
    "        :return: Aggregated feature vector, shape: [B, d_model]\n",
    "        \"\"\"\n",
    "        B, nvars_x, H, _ = x.shape\n",
    "        _, nvars_z, _, _ = z.shape\n",
    "\n",
    "        # Flatten last two dimensions: [B, nvars, H, d_model] -> [B, nvars, H * d_model]\n",
    "        flatten = nn.Flatten(start_dim=-2)\n",
    "        runoff = flatten(x)  # [B, nvars_x, H*d_model]#32,5,512\n",
    "        exogenous = flatten(z)  # [B, nvars_z, H*d_model]\n",
    "\n",
    "        # Add [CLS] token to the beginning of the query sequence\n",
    "        if self.use_cls:\n",
    "            cls_tokens = self.cls_token.expand(B, -1, -1, -1).squeeze(2)  # [B, 1, H*d_model]\n",
    "            runoff = torch.cat([cls_tokens, runoff], dim=1)  # [B, nvars_x+1, H*d_model]\n",
    "\n",
    "        # Project Query, Key, Value\n",
    "        q = self.q_proj(runoff)  # [B, nvars_q, d_model]\n",
    "        k = self.k_proj(exogenous)  # [B, nvars_k, d_model]\n",
    "        v = self.v_proj(runoff)  # [B, nvars_v, d_model]\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, -1, self.n_heads, self.head_dim).transpose(1, 2)  # [B, heads, nvars_q, head_dim]\n",
    "        k = k.view(B, -1, self.n_heads, self.head_dim).transpose(1, 2)  # [B, heads, nvars_k, head_dim]\n",
    "        v = v.view(B, -1, self.n_heads, self.head_dim).transpose(1, 2)  # [B, heads, nvars_v, head_dim]\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * (1.0 / (self.head_dim ** 0.5))  # [B, heads, nvars_q, nvars_k]\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).contiguous().view(B, -1, self.n_heads * self.head_dim)  # [B, nvars_q, d_model]\n",
    "        x = self.out_proj(x)  # [B, nvars_q, d_model]\n",
    "\n",
    "        # Normalize\n",
    "        x = self.norm(x)#32,6,64\n",
    "\n",
    "        # Take only the [CLS] token's representation as final output\n",
    "        if self.use_cls:\n",
    "            x = x[:, 0]  # [B, d_model]\n",
    "            x=self.linear(x)\n",
    "        else:\n",
    "            x = x.mean(dim=1)  # fallback: average over query tokens\n",
    "\n",
    "        return x\n",
    "model = Fredformer_backbone( in_seq_len, out_seq_len,ablation:=0, mlp_drop=0.3, use_nys=0, output=0, mlp_hidden=64, cf_dim=48,\n",
    "                 cf_depth=2, cf_heads=6, cf_mlp=128, cf_head_dim=32, cf_drop=0.2, c_in=6,\n",
    "                 patch_len=6, stride=6, d_model=64,\n",
    "                 head_dropout=0, padding_patch=None, individual=False, revin=False, affine=True, subtract_last=False).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e1d415-fc40-4924-a3c8-f1615c4f63d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1948837\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "learn_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df1ae38-d344-40ab-8d09-a36f52c8f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUnderestimationLoss(nn.Module):\n",
    "    def __init__(self, lambda_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_weight = lambda_weight\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        underestimation = torch.relu(target - output)\n",
    "        penalty = torch.atan(underestimation) ** 2\n",
    "        loss = self.lambda_weight * penalty.mean() + nn.MSELoss()(output, target)\n",
    "        return loss\n",
    "class FrequencyDomainLoss(nn.Module):\n",
    "    def __init__(self, dim=1):\n",
    "        super(FrequencyDomainLoss, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        output_fft = torch.fft.rfft(output, dim=self.dim)\n",
    "        target_fft = torch.fft.rfft(target, dim=self.dim)\n",
    "        loss = (output_fft - target_fft).abs().mean()\n",
    "        return loss\n",
    "\n",
    "loss_func = CustomUnderestimationLoss(lambda_weight=1.0)\n",
    "#loss_func = nn.MSELoss()\n",
    "#loss_func = FrequencyDomainLoss(dim=1)\n",
    "opt = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2175f08-3996-4ebf-9c24-104942abb15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [0/299]\t\tTrain loss: 0.404330 - Val loss: 0.219101\n",
      "\n",
      "Epoch [1/299]\t\tTrain loss: 0.259715 - Val loss: 0.233476\n",
      "\n",
      "Epoch [2/299]\t\tTrain loss: 0.253767 - Val loss: 0.204027\n",
      "\n",
      "Epoch [3/299]\t\tTrain loss: 0.250497 - Val loss: 0.242054\n",
      "\n",
      "Epoch [4/299]\t\tTrain loss: 0.274440 - Val loss: 0.225379\n",
      "\n",
      "Epoch [5/299]\t\tTrain loss: 0.254464 - Val loss: 0.221849\n",
      "\n",
      "Epoch [6/299]\t\tTrain loss: 0.252227 - Val loss: 0.221663\n",
      "\n",
      "Epoch [7/299]\t\tTrain loss: 0.248036 - Val loss: 0.207420\n",
      "\n",
      "Epoch [8/299]\t\tTrain loss: 0.241034 - Val loss: 0.224052\n",
      "\n",
      "Epoch [9/299]\t\tTrain loss: 0.244030 - Val loss: 0.206788\n",
      "\n",
      "Epoch [10/299]\t\tTrain loss: 0.237996 - Val loss: 0.203354\n",
      "\n",
      "Epoch [11/299]\t\tTrain loss: 0.235728 - Val loss: 0.199874\n",
      "\n",
      "Epoch [12/299]\t\tTrain loss: 0.234670 - Val loss: 0.218414\n",
      "\n",
      "Epoch [13/299]\t\tTrain loss: 0.235679 - Val loss: 0.207916\n",
      "\n",
      "Epoch [14/299]\t\tTrain loss: 0.234366 - Val loss: 0.201984\n",
      "\n",
      "Epoch [15/299]\t\tTrain loss: 0.234756 - Val loss: 0.202837\n",
      "\n",
      "Epoch [16/299]\t\tTrain loss: 0.234349 - Val loss: 0.194403\n",
      "\n",
      "Epoch [17/299]\t\tTrain loss: 0.230912 - Val loss: 0.202523\n",
      "\n",
      "Epoch [18/299]\t\tTrain loss: 0.228521 - Val loss: 0.204453\n",
      "\n",
      "Epoch [19/299]\t\tTrain loss: 0.234171 - Val loss: 0.200095\n",
      "\n",
      "Epoch [20/299]\t\tTrain loss: 0.232641 - Val loss: 0.206639\n",
      "\n",
      "Epoch [21/299]\t\tTrain loss: 0.227926 - Val loss: 0.206505\n",
      "\n",
      "Epoch [22/299]\t\tTrain loss: 0.225890 - Val loss: 0.205464\n",
      "\n",
      "Epoch [23/299]\t\tTrain loss: 0.225039 - Val loss: 0.195612\n",
      "\n",
      "Epoch [24/299]\t\tTrain loss: 0.224735 - Val loss: 0.204891\n",
      "\n",
      "Epoch [25/299]\t\tTrain loss: 0.223747 - Val loss: 0.203013\n",
      "\n",
      "Epoch [26/299]\t\tTrain loss: 0.223452 - Val loss: 0.199359\n",
      "\n",
      "Epoch [27/299]\t\tTrain loss: 0.224268 - Val loss: 0.194557\n",
      "Epoch 00028: reducing learning rate of group 0 to 3.0000e-04.\n",
      "\n",
      "Epoch [28/299]\t\tTrain loss: 0.218355 - Val loss: 0.195463\n",
      "\n",
      "Epoch [29/299]\t\tTrain loss: 0.216611 - Val loss: 0.197142\n",
      "\n",
      "Epoch [30/299]\t\tTrain loss: 0.215194 - Val loss: 0.198584\n",
      "\n",
      "Epoch [31/299]\t\tTrain loss: 0.213937 - Val loss: 0.199640\n",
      "\n",
      "Epoch [32/299]\t\tTrain loss: 0.213923 - Val loss: 0.196283\n",
      "\n",
      "Epoch [33/299]\t\tTrain loss: 0.214509 - Val loss: 0.206424\n",
      "\n",
      "Epoch [34/299]\t\tTrain loss: 0.213137 - Val loss: 0.198957\n"
     ]
    }
   ],
   "source": [
    "class EarlyStopping:\n",
    "\n",
    "    def __init__(self, patience, model_save_path, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.model_save_path = model_save_path\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "        self.best_epoch = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "\n",
    "    def __call__(self, epoch, model, validation_loss):\n",
    "        delta_loss = self.min_validation_loss - validation_loss\n",
    "        # Check if val loss is smaller than min loss\n",
    "        if delta_loss > self.min_delta:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            # Save best model\n",
    "            self.best_epoch = epoch\n",
    "            torch.save(model.state_dict(), self.model_save_path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Early Stopping.\")\n",
    "                print(f\"Save best model at epoch {self.best_epoch}\")\n",
    "                self.early_stop = True\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "# Reduce learning rate when validation loss stops improving\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.3, patience=lr_patience, verbose=True)\n",
    "\n",
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for X, y in data_loader:\n",
    "        X, y = X.float().to(device), y.float().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(X).squeeze()\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_avg_loss = total_loss / num_batches\n",
    "\n",
    "    return train_avg_loss\n",
    "\n",
    "\n",
    "def val_model(data_loader, model, loss_function):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.float().to(device), y.float().to(device)\n",
    "\n",
    "            output = model(X).squeeze()\n",
    "            total_loss += loss_function(output, y).item()\n",
    "\n",
    "    val_avg_loss = total_loss / num_batches\n",
    "\n",
    "    return val_avg_loss\n",
    "\n",
    "# Log losses for plotting\n",
    "all_losses = []\n",
    "\n",
    "# Initialize Early Stopping object\n",
    "early_stopper = EarlyStopping(patience=es_patience, model_save_path=model_save_path)\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(train_loader, model, loss_func, opt)\n",
    "    val_loss = val_model(val_loader, model, loss_func)\n",
    "    all_losses.append([train_loss, val_loss])\n",
    "\n",
    "    # Display\n",
    "    print(f\"\\nEpoch [{epoch}/{num_epochs-1}]\\t\\tTrain loss: {train_loss:.6f} - Val loss: {val_loss:.6f}\")\n",
    "\n",
    "    # EarlyStopping\n",
    "    early_stopper(epoch, model, val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        break\n",
    "    # Adjust learning rate\n",
    "    lr_scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a26e4-49ed-449d-8b05-b356e02656f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"loss curves\", size=18, y=1.1)\n",
    "plt.plot(all_losses, label=[\"Train loss\", \"Val loss\"])\n",
    "plt.xlabel(\"Epoch\", fontsize=13)\n",
    "plt.ylabel(\"MSE\", fontsize=13)\n",
    "plt.legend(loc=\"upper right\", fontsize=10)\n",
    "plt.show()\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "def predict(data_loader, model):\n",
    "    pred, true = torch.tensor([]), torch.tensor([])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.double()\n",
    "            y = y.double()\n",
    "            y_pred = model(X.float().to(device))\n",
    "            pred=pred.to(device).double()\n",
    "            true =true.to(device).double()\n",
    "            pred = torch.cat((pred, y_pred.to(device)), 0).double()\n",
    "            true = torch.cat((true, y.to(device)), 0).double()\n",
    "\n",
    "    return pred, true\n",
    "    \n",
    "y_pred_tensor, y_test_tensor = predict(test_loader, model)\n",
    "y_pred, y_test = y_pred_tensor.cpu().numpy(), y_test_tensor.cpu().numpy()\n",
    "y_pred = y_pred.squeeze()\n",
    "# Inverse the transformation\n",
    "y_pred_inv1 = scaler_qrate.inverse_transform(y_pred)\n",
    "y_test_inv1 = scaler_qrate.inverse_transform(y_test)\n",
    "y_pred_inv = np.expm1(y_pred_inv1)\n",
    "y_test_inv = np.expm1(y_test_inv1)\n",
    "\n",
    "# Hours ahead to predict\n",
    "forecast_length = 24\n",
    "\n",
    "truth = y_test_inv[:, :] # 取所有时间步长的真实值\n",
    "forecast = y_pred_inv[:, :]  # 取所有时间步长的预测值\n",
    "print(truth.shape)\n",
    "# 计算误差\n",
    "diff = np.subtract(truth, forecast)\n",
    "\n",
    "# 计算指标\n",
    "mae = np.mean(np.abs(diff))  # MAE\n",
    "mse = np.mean(np.square(diff))  # MSE\n",
    "rmse = np.sqrt(mse)  # RMSE\n",
    "\n",
    "# NSE\n",
    "num = np.sum(np.square(diff))\n",
    "den = np.sum(np.square(np.subtract(truth, truth.mean())))\n",
    "nse = 1 - (num / den)\n",
    "\n",
    "# R^2\n",
    "numerator = np.square(np.sum((truth - truth.mean()) * (forecast - forecast.mean())))\n",
    "denominator = np.sum(np.square(truth - truth.mean())) * np.sum(np.square(forecast - forecast.mean()))\n",
    "r_squared = numerator / denominator\n",
    "\n",
    "# RSR\n",
    "rsr = rmse / np.std(truth)\n",
    "\n",
    "# Pbias\n",
    "pbias = np.mean(diff / truth) * 100\n",
    "\n",
    "# 输出结果\n",
    "print(f\"Overall forecast MAE : {mae:.4f}\")\n",
    "print(f\"Overall forecast MSE: {rmse:.4f}\")\n",
    "print(f\"Overall forecast NSE: {nse:.4f}\")\n",
    "print(f\"Overall forecast R^2: {r_squared:.4f}\")\n",
    "print(f\"Overall forecast RSR: {rsr:.4f}\")\n",
    "print(f\"Overall forecast Pbias: {pbias:.2f}%\")\n",
    "\n",
    "truth = y_test_inv[:, 0]\n",
    "forecast = y_pred_inv[:, 0]\n",
    "\n",
    "plt.title(f\"{forecast_length}-Hour Ahead Forecasting\", size=16, y=1.1)\n",
    "plt.plot(truth, label=\"Ground Truth\", color=\"teal\")\n",
    "plt.plot(forecast, label=\"Prediction\", color=\"darkred\")\n",
    "plt.xlabel(\"Observation\")\n",
    "plt.legend(fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de7e9e-5cd9-49e2-855a-1a3d2aaf211b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
